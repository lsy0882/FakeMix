wandb:
  login: 
    key: "" ### Login key / Don't change
  init: ### Ref: https://docs.wandb.ai/ref/python/init
    project: "FakeMix" ### Dont't change
    entity: "" ### Your wandb profile name (=id)
    save_code: true ### Don't change
    group: "" ### Don't change / Ref: https://docs.wandb.ai/guides/runs/grouping
    job_type: "test" ### "data-preprocess", "train", "test", "visualize" etc...
    tags: ["Baseline_1", "Unimodal", "Video", "Xception"] ### [Network, Size, etc...]
    name: "Baseline_1_Unimodal_Video_Xception" ### "Network"_"Size"_"Version" | Version policy: v{Architecture change}_{Method/Block change}_{Layer/Minor change}
    notes: "Test [Baseline_1_Unimodal_Video_Xception] to our benchmark" ### Insert changes(plz write details)
    dir: "./wandb" ### Don't change
    resume: "auto" ### Don't change
    reinit: false ### Don't change
    magic: null ### Don't change
    config_exclude_keys: [] ### Don't change
    config_include_keys: [] ### Don't change
    anonymous: null ### Don't change
    mode: "online" ### Don't change
    allow_val_change: true ### Don't change
    force: false ### Don't change
    sync_tensorboard: false ### Don't change
    monitor_gym: false ### Don't change
    config:
      dataset:
        data_path: "/home/lsy/laboratory/Research/FakeMix/data"
      dataloader:
        batch_size: 32
        pin_memory: true
        num_workers: 12
      model:
        num_classes: 2
      criterion: ### Ref: https://pytorch.org/docs/stable/nn.html#loss-functions
        name: "CrossEntropyLoss" ### Choose a torch.nn's class(=attribute) e.g. ["CrossEntropyLoss", "MSELoss", "Custom", ...] / You can build your criterion :)
      optimizer: ### Ref: https://pytorch.org/docs/stable/optim.html#algorithms
        name: "Adam" ### Choose a torch.optim's class(=attribute) e.g. ["Adam", "AdamW", "SGD", ...] / You can build your optimizer :)
        Adam: ### Add or modify instance & args using reference link
          lr: 1.0e-1
          weight_decay: 1.0e-2
        AdamW:
          lr: 2.0e-3
          weight_decay: 1.0e-4
        SGD:
          lr: 2.0e-3
          momentum: 0.9
          weight_decay: 1.0e-4
        Custom:
          custom_arg1:
          custom_arg2:
      scheduler: ### Ref(+ find "How to adjust learning rate"): https://pytorch.org/docs/stable/optim.html#algorithms
        name: "StepLR" ### Choose a torch.optim.lr_scheduler's class(=attribute) e.g. ["StepLR", "ReduceLROnPlateau", "Custom"] / You can build your scheduler :)
        StepLR: ### Add or modify instance & args using reference link
          step_size: 5
          gamma: 0.9
        ReduceLROnPlateau:
          mode: "min"
          min_lr: 1.0e-10
          factor: 0.9
          patience: 5
        Custom:
          custom_arg1:
          custom_arg2:
      engine:
        epoch: 200
        gpuid: "0" ### "0"(single-gpu) or "0, 1" (multi-gpu)